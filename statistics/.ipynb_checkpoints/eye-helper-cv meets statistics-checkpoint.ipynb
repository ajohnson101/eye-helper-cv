{
 "metadata": {
  "name": "",
  "signature": "sha256:3c57015f9f227f98196137613418c93b7ce15c8972d8292ac958831e8adcd23d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##eye-helper-cv meets statistics\n",
      "July 23, 2014 \n",
      "\n",
      "Hello there!\n",
      "\n",
      "The goal of this ipython notebook is to document our statistical analysis for comparing object tracking methods in eye-helper-cv. \n",
      "\n",
      "### Relevant datasets in this notebook\n",
      "- Cookie \n",
      "- Cereal \n",
      "\n",
      "### Methods we're comparing\n",
      "- SIFT\n",
      "- SURF\n",
      "- ORB\n",
      "- BRISK\n",
      "\n",
      "###Contents include:\n",
      "- Handy functions (originally in compare_kpd.py)\n",
      "- ANOVA testing to compare performance between the four methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Importing relevant things"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cv2\n",
      "import numpy as np \n",
      "import csv\n",
      "import pickle\n",
      "import scipy.ndimage \n",
      "import scipy.stats\n",
      "import os\n",
      "import math\n",
      "import matplotlib.pyplot as plt  \n",
      "import pprint as pp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## ANOVA (analysis of variance) Testing - Round 1\n",
      "\n",
      "Comparing everything to itself to check if there's a statistically significant difference and it's not just because one dataset is \"wonky\" in particular. \n",
      "- SURF to SURF\n",
      "- SIFT to SIFT\n",
      "- BRISK to BRISK\n",
      "- ORB to ORB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## ANOVA testing, round 1\n",
      "## Original code in this chunk is from the superANOVA function in compare_kpd.py\n",
      "\n",
      "dstr = 'cookie'\n",
      "framerange = [124, 288]\n",
      "\n",
      "accuracy = False #default False \n",
      "distance = False #default False\n",
      "\n",
      "data = {}\n",
      "correct_centers = {}\n",
      "testables = {}\n",
      "test_res = {}\n",
      "\n",
      "#Loads the pickles which have all the possible data that we could want\n",
      "for trial in range(framerange[0], framerange[1], 20):\n",
      "    data[trial] ={}\n",
      "    for mstr in methods:\n",
      "        data[trial][mstr]= pickle.load(open('../OT-res/compare_kpd_plots/%s_%s.p' % (dstr, mstr), 'rb'))[trial]\n",
      "        correct_centers[mstr] = []\n",
      "        testables[mstr] = []\n",
      "\n",
      "# generates the accuracy and normalized distance from center information that we are interested in \n",
      "for trial in data:\n",
      "    d_from_c = {}\n",
      "    for method in data[trial]:\n",
      "        try:\n",
      "            trialdata = data[trial][method]\n",
      "            frames = [int(x) - trial for x in trialdata['frame numbers']]\n",
      "            d_from_c[method] = ([trialdata['distance from center'][x]/trialdata['hypotenuse'][x] for x in range(len(trialdata['distance from center']))])\n",
      "            #variable is actually the *percent* of correctly matched centers not the number   \n",
      "\n",
      "            if accuracy:\n",
      "                testables[method].append(len([x for x in trialdata['c_match'] if x is True]) / len(frames) * 100)\n",
      "        except:\n",
      "            print(\"If only I hadn't failed!\")\n",
      "\n",
      "    #If we are using this to compare distances use the relevant data generated above, do this for every trial\n",
      "    if distance:\n",
      "        f, p = scipy.stats.f_oneway(*[d_from_c[methods] for methods in d_from_c]) \n",
      "        test_res[trial] = p \n",
      "\n",
      "# If we are interested in comparing accuracy between method use relevant data from above, do this only once\n",
      "if accuracy:\n",
      "    f,p = scipy.stats.f_oneway(*[testables[methods] for methods in testables])\n",
      "    test_res = p\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## ANOVA Test Results\n",
      "\n",
      "|       | SIFT  | SURF  | ORB   | BRISK |\n",
      "|-------|-------|-------|-------|-------|\n",
      "| SIFT  |-------|       |-------|-------|\n",
      "| SURF  |       |       |-------|-------|\n",
      "| ORB   |       |       |       |       |\n",
      "| BRISK |       |       |       |       |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Violating most ANOVA test and p/t-test assumptions...\n",
      "- Need more videos... ideally with different products and in different environments (times of day, locations)\n",
      "- Perhaps stacked box plots?\n",
      "### Some interesting things\n",
      "- Individual box plots at different offsets for different methods\n",
      "- Box plots will either overlap or not... maybe some visual indication, but might not be the most interpretable\n",
      "- Another graph that might be more interesting - pick the top 2 methods, take the difference between those two methods' performances (i.e. distance variable probably works better for this than the accuracy variable)\n",
      "- 15 frames out, compare error of sift and error of surf, and get the delta between things ... and make a box plot of those numbers\n",
      "- telling us when different methods worked better than other methods\n",
      "- since box plots use standard deviations, they're less likely to be violated by these assumptions\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}